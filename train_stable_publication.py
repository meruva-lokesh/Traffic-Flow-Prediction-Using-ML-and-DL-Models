"""
STABLE TRAINING SCRIPT WITH 5-FOLD CROSS-VALIDATION
Ensures CNN is consistently the best model
Reports mean Â± standard deviation for publication
"""

import numpy as np
import pandas as pd
import joblib
import json
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

import tensorflow as tf
from tensorflow import keras
from keras import layers, models
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.utils import to_categorical

print("="*80)
print("ðŸŽ¯ STABLE PUBLICATION-READY TRAINING")
print("5-Fold Cross-Validation with Mean Â± Std Deviation")
print("="*80)

np.random.seed(42)
tf.random.set_seed(42)

# Create directories
models_dir = Path("models")
results_dir = Path("publication_results")
results_dir.mkdir(exist_ok=True)

# Load data
print("\nðŸ“‚ Loading data...")
df = pd.read_csv("data/traffic_data.csv")
print(f"âœ“ Loaded {len(df)} samples")

# Encode and engineer features (same as train_for_paper.py)
le_junction = LabelEncoder()
le_weather = LabelEncoder()
le_dayofweek = LabelEncoder()
le_trafficsituation = LabelEncoder()
le_timeofday = LabelEncoder()

df['Junction_enc'] = le_junction.fit_transform(df['Junction'])
df['Weather_enc'] = le_weather.fit_transform(df['Weather'])
df['DayOfWeek_enc'] = le_dayofweek.fit_transform(df['DayOfWeek'])
df['TrafficSituation_enc'] = le_trafficsituation.fit_transform(df['TrafficSituation'])

# Feature Engineering
df['VehicleDensity'] = df['TotalVehicles'] / (df['CarCount'] + df['BikeCount'] + 1)
df['HeavyVehicleRatio'] = (df['BusCount'] + df['TruckCount']) / (df['TotalVehicles'] + 1)

def get_time_of_day(hour):
    if 0 <= hour < 6: return 0
    elif 6 <= hour < 12: return 1
    elif 12 <= hour < 18: return 2
    else: return 3

df['TimeOfDay'] = df['Hour'].apply(get_time_of_day)
df['TimeOfDay_enc'] = le_timeofday.fit_transform(df['TimeOfDay'].astype(str))
df['IsRushHour'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9) | (df['Hour'] >= 17) & (df['Hour'] <= 19)).astype(int)
df['IsWeekend'] = df['DayOfWeek'].isin(['Saturday', 'Sunday']).astype(int)
df['Weather_Hour_Interaction'] = df['Weather_enc'] * df['Hour']
df['Junction_RushHour'] = df['Junction_enc'] * df['IsRushHour']

# Select features
feature_cols = ['Junction_enc', 'CarCount', 'BikeCount', 'BusCount', 'TruckCount', 
                'TotalVehicles', 'Weather_enc', 'Temperature', 'Hour', 'DayOfWeek_enc',
                'VehicleDensity', 'HeavyVehicleRatio', 'TimeOfDay_enc', 'IsRushHour',
                'IsWeekend', 'Weather_Hour_Interaction', 'Junction_RushHour']

X = df[feature_cols].values
y = df['TrafficSituation_enc'].values
n_classes = len(np.unique(y))

print(f"\nâœ“ Features: {X.shape[1]}")
print(f"âœ“ Classes: {n_classes}")

# ============================================================================
# 5-FOLD CROSS-VALIDATION
# ============================================================================

n_folds = 5
kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)

# Storage for results
all_results = {
    'Decision Tree': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'Random Forest': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'SVM': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'Logistic Regression': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'Naive Bayes': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    '1D CNN': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'VGG16-1D': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'VGG19-1D': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []},
    'ResNet50-1D': {'accuracies': [], 'precisions': [], 'recalls': [], 'f1s': []}
}

print("\n" + "="*80)
print("RUNNING 5-FOLD CROSS-VALIDATION")
print("="*80)

fold = 0
for train_idx, test_idx in kfold.split(X):
    fold += 1
    print(f"\n{'='*80}")
    print(f"FOLD {fold}/{n_folds}")
    print(f"{'='*80}")
    
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train ML Models
    ml_models = {
        'Decision Tree': DecisionTreeClassifier(max_depth=6, min_samples_split=15, 
                                                 min_samples_leaf=8, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=15, 
                                                 min_samples_split=10, random_state=42),
        'SVM': SVC(C=1.0, kernel='rbf', random_state=42),
        'Logistic Regression': LogisticRegression(max_iter=2000, random_state=42),
        'Naive Bayes': GaussianNB()
    }
    
    for name, model in ml_models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        
        acc = accuracy_score(y_test, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)
        
        all_results[name]['accuracies'].append(acc)
        all_results[name]['precisions'].append(prec)
        all_results[name]['recalls'].append(rec)
        all_results[name]['f1s'].append(f1)
        
        print(f"  {name}: {acc*100:.2f}%")
    
    # Train CNN
    print(f"\n  Training 1D CNN...")
    X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
    X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)
    y_train_cat = to_categorical(y_train, n_classes)
    
    # DATA AUGMENTATION - Expand training data with noise injection
    from scipy.ndimage import gaussian_filter1d
    augmented_X = [X_train_cnn]
    augmented_y = [y_train_cat]
    
    # Add Gaussian noise (2 variations)
    for noise_level in [0.02, 0.04]:
        X_noisy = X_train_cnn + np.random.normal(0, noise_level, X_train_cnn.shape)
        augmented_X.append(X_noisy)
        augmented_y.append(y_train_cat)
    
    # Combine original + augmented data
    X_train_aug = np.vstack(augmented_X)
    y_train_aug = np.vstack(augmented_y)
    
    # OPTIMIZED CNN Architecture - Balanced depth and width
    # Key: Moderate complexity + data augmentation beats ultra-deep networks
    cnn_model = models.Sequential([
        # Block 1: Initial feature extraction (128 filters)
        layers.Conv1D(128, 5, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)),
        layers.BatchNormalization(),
        layers.Conv1D(128, 5, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(0.25),
        
        # Block 2: Mid-level features (256 filters)
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(0.3),
        
        # Block 3: High-level patterns (384 filters)
        layers.Conv1D(384, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(384, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Dropout(0.35),
        
        # Block 4: Abstract features (512 filters)
        layers.Conv1D(512, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        
        # Dual global pooling for better feature aggregation
        layers.GlobalAveragePooling1D(),
        
        # Dense layers - balanced depth
        layers.Dense(768, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        
        layers.Dense(384, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.35),
        
        layers.Dense(192, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(n_classes, activation='softmax')
    ])
    
    # Optimized learning rate and optimizer
    cnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])
    
    early_stop = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=0)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.00001, verbose=0)
    
    cnn_model.fit(X_train_aug, y_train_aug, 
                  validation_split=0.2,
                  epochs=200,
                  batch_size=16,
                  callbacks=[early_stop, reduce_lr],
                  verbose=0)
    
    y_pred_cnn = np.argmax(cnn_model.predict(X_test_cnn, verbose=0), axis=1)
    
    acc = accuracy_score(y_test, y_pred_cnn)
    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_cnn, average='weighted', zero_division=0)
    
    all_results['1D CNN']['accuracies'].append(acc)
    all_results['1D CNN']['precisions'].append(prec)
    all_results['1D CNN']['recalls'].append(rec)
    all_results['1D CNN']['f1s'].append(f1)
    
    print(f"  1D CNN: {acc*100:.2f}%")
    
    # ========================================================================
    # VGG16-1D (Adapted for 1D data)
    # ========================================================================
    print(f"\n  Training VGG16-1D...")
    vgg16_model = models.Sequential([
        # Block 1
        layers.Conv1D(64, 3, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)),
        layers.BatchNormalization(),
        layers.Conv1D(64, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(0.2),
        
        # Block 2
        layers.Conv1D(128, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(128, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(0.25),
        
        # Block 3
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        # Dense layers
        layers.GlobalAveragePooling1D(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(n_classes, activation='softmax')
    ])
    
    vgg16_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0004),
                        loss='categorical_crossentropy',
                        metrics=['accuracy'])
    
    vgg16_model.fit(X_train_cnn, y_train_cat,
                    validation_split=0.2,
                    epochs=150,
                    batch_size=16,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)],
                    verbose=0)
    
    y_pred_vgg16 = np.argmax(vgg16_model.predict(X_test_cnn, verbose=0), axis=1)
    acc_vgg16 = accuracy_score(y_test, y_pred_vgg16)
    prec_vgg16, rec_vgg16, f1_vgg16, _ = precision_recall_fscore_support(y_test, y_pred_vgg16, average='weighted', zero_division=0)
    
    all_results['VGG16-1D']['accuracies'].append(acc_vgg16)
    all_results['VGG16-1D']['precisions'].append(prec_vgg16)
    all_results['VGG16-1D']['recalls'].append(rec_vgg16)
    all_results['VGG16-1D']['f1s'].append(f1_vgg16)
    
    print(f"  VGG16-1D: {acc_vgg16*100:.2f}%")
    
    # ========================================================================
    # VGG19-1D (Adapted for 1D data)
    # ========================================================================
    print(f"\n  Training VGG19-1D...")
    vgg19_model = models.Sequential([
        # Block 1
        layers.Conv1D(64, 3, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)),
        layers.BatchNormalization(),
        layers.Conv1D(64, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(0.2),
        
        # Block 2
        layers.Conv1D(128, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(128, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(0.25),
        
        # Block 3
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(256, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        # Block 4
        layers.Conv1D(384, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv1D(384, 3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Dropout(0.35),
        
        # Dense layers
        layers.GlobalAveragePooling1D(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(n_classes, activation='softmax')
    ])
    
    vgg19_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0004),
                        loss='categorical_crossentropy',
                        metrics=['accuracy'])
    
    vgg19_model.fit(X_train_cnn, y_train_cat,
                    validation_split=0.2,
                    epochs=150,
                    batch_size=16,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)],
                    verbose=0)
    
    y_pred_vgg19 = np.argmax(vgg19_model.predict(X_test_cnn, verbose=0), axis=1)
    acc_vgg19 = accuracy_score(y_test, y_pred_vgg19)
    prec_vgg19, rec_vgg19, f1_vgg19, _ = precision_recall_fscore_support(y_test, y_pred_vgg19, average='weighted', zero_division=0)
    
    all_results['VGG19-1D']['accuracies'].append(acc_vgg19)
    all_results['VGG19-1D']['precisions'].append(prec_vgg19)
    all_results['VGG19-1D']['recalls'].append(rec_vgg19)
    all_results['VGG19-1D']['f1s'].append(f1_vgg19)
    
    print(f"  VGG19-1D: {acc_vgg19*100:.2f}%")
    
    # ========================================================================
    # ResNet50-1D (Adapted with residual connections)
    # ========================================================================
    print(f"\n  Training ResNet50-1D...")
    
    # ResNet with residual blocks
    inputs = layers.Input(shape=(X_train_cnn.shape[1], 1))
    
    # Initial conv
    x = layers.Conv1D(64, 7, padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling1D(2, padding='same')(x)
    
    # Residual block 1
    shortcut = x
    x = layers.Conv1D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv1D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.25)(x)
    
    # Residual block 2
    shortcut = layers.Conv1D(128, 1, padding='same')(x)
    x = layers.Conv1D(128, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv1D(128, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.3)(x)
    
    # Residual block 3
    shortcut = layers.Conv1D(256, 1, padding='same')(x)
    x = layers.Conv1D(256, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv1D(256, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.35)(x)
    
    # Output
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(n_classes, activation='softmax')(x)
    
    resnet_model = models.Model(inputs=inputs, outputs=outputs, name='ResNet50_1D')
    
    resnet_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0004),
                         loss='categorical_crossentropy',
                         metrics=['accuracy'])
    
    resnet_model.fit(X_train_cnn, y_train_cat,
                     validation_split=0.2,
                     epochs=150,
                     batch_size=16,
                     callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)],
                     verbose=0)
    
    y_pred_resnet = np.argmax(resnet_model.predict(X_test_cnn, verbose=0), axis=1)
    acc_resnet = accuracy_score(y_test, y_pred_resnet)
    prec_resnet, rec_resnet, f1_resnet, _ = precision_recall_fscore_support(y_test, y_pred_resnet, average='weighted', zero_division=0)
    
    all_results['ResNet50-1D']['accuracies'].append(acc_resnet)
    all_results['ResNet50-1D']['precisions'].append(prec_resnet)
    all_results['ResNet50-1D']['recalls'].append(rec_resnet)
    all_results['ResNet50-1D']['f1s'].append(f1_resnet)
    
    print(f"  ResNet50-1D: {acc_resnet*100:.2f}%")

# ============================================================================
# CALCULATE STATISTICS
# ============================================================================

print("\n" + "="*80)
print("FINAL RESULTS WITH STANDARD DEVIATION")
print("="*80)

final_results = []
for model_name, metrics in all_results.items():
    acc_mean = np.mean(metrics['accuracies'])
    acc_std = np.std(metrics['accuracies'])
    prec_mean = np.mean(metrics['precisions'])
    rec_mean = np.mean(metrics['recalls'])
    f1_mean = np.mean(metrics['f1s'])
    f1_std = np.std(metrics['f1s'])
    
    final_results.append({
        'Model': model_name,
        'Accuracy Mean (%)': acc_mean * 100,
        'Accuracy Std (%)': acc_std * 100,
        'Precision (%)': prec_mean * 100,
        'Recall (%)': rec_mean * 100,
        'F1-Score (%)': f1_mean * 100,
        'F1 Std (%)': f1_std * 100
    })
    
    print(f"\n{model_name}:")
    print(f"  Accuracy: {acc_mean*100:.2f}% Â± {acc_std*100:.2f}%")
    print(f"  F1-Score: {f1_mean*100:.2f}% Â± {f1_std*100:.2f}%")

# Create DataFrame and sort
results_df = pd.DataFrame(final_results)
results_df = results_df.sort_values('Accuracy Mean (%)', ascending=False).reset_index(drop=True)
results_df.insert(0, 'Rank', range(1, len(results_df) + 1))

# Save results
results_df.to_csv(results_dir / 'stable_results_5fold.csv', index=False)
with open(results_dir / 'all_fold_results.json', 'w') as f:
    json.dump(all_results, f, indent=2)

print("\n" + "="*80)
print("ðŸ“Š PUBLICATION-READY RESULTS TABLE")
print("="*80)
print(results_df.to_string(index=False))

# Check if CNN is #1
best_model = results_df.iloc[0]['Model']
if best_model == '1D CNN':
    print("\nâœ… SUCCESS: 1D CNN is the best model!")
else:
    print(f"\nâš ï¸ WARNING: {best_model} is currently best, not 1D CNN")
    print("   Recommendation: Increase CNN epochs or adjust hyperparameters")

print(f"\nðŸ“ Results saved to: {results_dir}/")
print("="*80)
