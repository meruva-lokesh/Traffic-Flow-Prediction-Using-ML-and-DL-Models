\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Deep Learning for Traffic Flow Prediction: A Comprehensive Comparative Study with Data Augmentation\\}

\author{
\IEEEauthorblockN{Gorantla Arshitha}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{and Engineering} \\
\textit{Koneru Lakshmaiah Education}\\
Foundation, Vaddeswaram, India \\
2200031594@kluniversity.in}
\and
\IEEEauthorblockN{Meruva Lokesh}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{and Engineering} \\
\textit{Koneru Lakshmaiah Education}\\
Foundation, Vaddeswaram, India \\
2200031499@kluniversity.in}
\and
\IEEEauthorblockN{Ittela Bhavana Sriya}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{and Engineering} \\
\textit{Koneru Lakshmaiah Education}\\
Foundation, Vaddeswaram, India \\
2200031612@kluniversity.in}
\and
\IEEEauthorblockN{P. V. V. S. Srinivas}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{and Engineering} \\
\textit{Koneru Lakshmaiah Education} \\
Foundation, Vaddeswaram, India \\
cnu.pvvs@kluniversity.in}
}

\maketitle

\begin{abstract}
Intelligent transport systems and smart cities require accurate predictions of traffic to function well regardless of local conditions. In this paper, we compare many different machine learning models to create classifications of vehicle counts. We will be using five traditional algorithms (Decision tree (DT), Random forest (RF), Support vector machine (SVM), Logistic regression (LR), Naive bayes (NB)) and four deep learning models (1D convolutional neural network (1D CNN), VGG16 (1D), VGG19 (1D), and ResNet50 (1D)). We have created an optimized model for this application, a 1D CNN, that uses 17 features from a dataset of over 5000 samples of traffic count data and has been designed specifically to work with temporally-based tabular data. To augment our dataset, we used Gaussian noise to multiply our training dataset. To evaluate the overall performance of all the models we used 5-fold cross-validation with 92.16\% $\pm$ 0.72\% of our CNN correctly classifying traffic into four severity categories: Low, Medium, High, and Severe. In comparison with the other models, the CNN exceeded the performance of the Random Forest (90.86\%), Decision tree (90.68\%), and the VGG16 (1D), VGG19 (1D), and ResNet50 (1D) transfer learning models —by more than two percentage points each. The key innovations of our model are a balanced 4 block architecture with a progressive filter distribution for increasing the number of filters (128, 256, 384, 512), effective regularization (e.g., BatchNorm and progressive dropout), and the results from our research indicate that architectures developed specifically for the task of classifying traffic are going to outperform architectures adapted from other applications (e.g., images) for classifying temporal data (i.e. 1D) in real time (8.5 ms).
\end{abstract}
\vspace{0.1cm}
\begin{IEEEkeywords}
Traffic Flow Prediction, Convolutional Neural Network, Deep Learning, Machine Learning, Transfer Learning, Data Augmentation, Cross Validation, Intelligent Transport Systems.
\end{IEEEkeywords}

\section{Introduction}
Urban traffic congestion presents a substantial challenge to cities today. The problems caused by traffic congestion have negative impacts such as economic losses, increased pollution, and a lower quality of life for citizens \cite{ref1, ref4}. By predicting traffic flow better, cities can manage traffic proactively, plan better routes for vehicles, and allocate resources more efficiently in "smart" city designs \cite{ref11}. There have been significant advancements in the use of machine learning and deep learning techniques for the purpose of predicting future traffic patterns \cite{ref2, ref7, ref19}. These new technological advances compare favorably with traditional statistical methods previously used to predict future traffic patterns.

\subsection{Background and Motivation}

For intelligent transportation systems (ITS) and a modern smart city infrastructure, accurate prediction of traffic in real time is critical \cite{ref8, ref10}. Due to the complexity and non-linearity of actual traffic patterns and the inability of traditional statistical techniques like the ARIMA and Kalman filter to provide accurate predictions, machine learning techniques have become the preferred vehicle for modeling traffic data \cite{ref1}. Some examples of machine learning algorithms that have improved the predictive capabilities for non-linear relationships within traffic data include support vector machines, random forests, and ensemble methods \cite{ref6, ref7}. In addition, various deep learning architectures, such as convolutional neural networks (CNNs) \cite{ref9, ref17}, long short-term memory networks (LSTMs) \cite{ref13, ref18}, and hybrids of both CNNs and LSTMs \cite{ref9, ref14}, have exhibited incredible ability to capture the hierarchical features and temporal dependencies within a traffic data set. In addition, the application of some advanced modeling techniques, such as graph convolutional networks \cite{ref15, ref16}, and spatio-temporal metadata learning \cite{ref15}, have yielded outstanding accuracy in the prediction of traffic data by representing traffic as graph-structured data. In addition to the aforementioned advances in the prediction of traffic data, improving the confidence of the prediction has also benefitted from the incorporation of weather data \cite{ref20} and the application of bio-inspired optimization algorithms \cite{ref9}.


\subsection{Research Gap}

Limitations exist in many traffic forecast-related research analyses despite much effort \cite{ref3, ref5}:

\begin{itemize}
    % \item Most studies focus on single model approaches without comprehensive comparison \cite{ref1, ref7, ref19}, failing to provide systematic evaluation across diverse algorithms
    % \item Transfer learning models (VGG, ResNet) are often applied without evaluating their suitability for temporal tabular data \cite{ref12}, as these architectures were originally designed for image recognition tasks
    % \item Statistical validation through k-fold cross-validation is rarely performed \cite{ref3, ref5}, limiting the reliability and generalizability of reported results
    % \item Systematic comparison between traditional ML and deep learning approaches is limited \cite{ref7, ref10}, making it difficult to determine optimal model selection for specific scenarios
    % \item Data augmentation strategies specific to traffic data remain unexplored \cite{ref9, ref18}, despite their proven effectiveness in other domains
    \item Most research methods are single model based, and comparisons between methods (comprehensive in nature) were not done \cite{ref1, ref7, ref19}; thus, there is little systematic evaluation of the algorithms as they vary across multiple domains.
    \item The transfer learning models (VGG, ResNet) used were mainly applied on temporal tabular data without an assessment as to whether these designs would yield comparable results with the original design (image recognition was the original intent of the architectures) \cite{ref12}.
    \item Statistical validation via k-fold cross-validation is rarely used in the analyses, leaving much of the data being presented as not reliable / valid or generalizable \cite{ref3, ref5}.
    \item A limited number of comparative studies between classical / traditional ML and deep learning approaches have been completed \cite{ref7, ref10}; therefore, it is difficult to determine the 'best' model choice in any case.
\end{itemize}

\subsection{Contributions}

This research paper provides the following contributions to address the identified research gaps within traffic flow prediction:

\begin{enumerate}
    \item \textbf{Comprehensive Comparison}: A review of 9 different models (5 traditional Machine Learning \cite{ref6, ref19}, and 4 Deep Learning \cite{ref9, ref13, ref17, ref18})) with a rigorous 5-fold cross-validation was used to make this the most comprehensive comparative study on traffic flow prediction. 
    \item \textbf{Optimized Architecture}: A novel 1D CNN-based architecture that uses a balanced amount of depth, width, and regularization to achieve an accuracy rate of 92.16\% \cite{ref17} this architecture was specifically designed to operate on temporal tabular data rather than be adapted from architectures built for image recognition.
    \item \textbf{Data Augmentation}: An injection strategy using Gaussian noise was created to increase the number of available training samples 3x \cite{ref9, ref18}. This addresses the challenge of data scarcity in traffic flow predictions. 
    \item \textbf{Transfer Learning Analysis}: Empirical evidence demonstrates that the image models pre-trained using transfer learning \cite{ref12} perform poorly compared to architecture-specific designs built for 1D temporal data; this challenges the traditional view of the transfer learning applicability.
    \item \textbf{Statistical Validation}: This research uses rigorous cross-validation \cite{ref3, ref5} to ensure the reproducibility and statistical significance of the results.
    \item \textbf{Practical Implementation}: An open-source implementation is appropriate for real-time deployment (inference ¡ 10ms) \cite{ref2, ref4, ref11}; this allows for immediate implementation into smart city infrastructure.
\end{enumerate}

This paper continues with an examination of the related literature (section II), followed by an overview of currently available works (section III), a description of methodology (section IV), a description of the architectural design (section V), a description of the implementation (section VI), a description of experimental results (section VII), and a conclusion with future directions (section VIII).

\section{Literature Review}

\subsection{Overview of Traffic Flow Prediction}

For more than thirty years, traffic flow prediction has been an area of research and now covers everything from simple statistical models to advanced deep learning architectures for predicting traffic flow. The earlier approaches to predicting traffic flow predominantly utilized time-series analysis methodologies (i.e., ARIMA and/or Kalman filter) which generally assume a linear relationship and, therefore, require stationary conditions in the traffic patterns over time. Many of the real-world traffic datasets will exhibit the following types of characteristics that will create problems for traditional statistical methods: non-linear dynamics; seasonal patterns; complex interdependencies.

\subsection{Machine Learning Era}

In the 2000s, the introduction of machine learning algorithms represented a large leap forward. Support Vector Machines (SVMs) were effective in modeling non-linear patterns using kernel methods. Decision Trees and combination methods such as Random Forest were useful for creating easily understood models that captured complex interactions between features. However, these techniques require a significant amount of manual truth engineering and have difficulty capturing the temporal dependencies associated with traffic data.

\subsection{Deep Learning Revolution}

Due to the advancements made possible by deep learning, particularly regarding computer vision and Natural Language Processing (NLP), we have greatly improved how traffic is predicted. The emergence of recurrent neural networks (RNNs) and long short-term memory (LSTM) networks made it possible for us to capture sequential dependencies within data. The development of convolutional neural networks (CNNs) has provided us with the ability to extract both spatial/temporal features from large dimensional traffic matrices. More recently, attention mechanisms, as represented via transformers, have proven to have the ability to capture long-distance dependencies.

\subsection{Current Challenges}

These advancements have not been without their challenges:
\begin{itemize}
    \item \textbf{Model Selection}: Few recommendations exist for selecting suitable architectures to address a given traffic prediction problem;
    \item \textbf{Transfer Learning}: Uncertain if pre-trained models for computer vision may be applied to traffic data;
    \item \textbf{Data Scarcity}: Many traffic agencies have limited labeled data, necessitating data augmentation strategiesMany traffic agencies have little labeled data available so will need to use augmentation methods;
    \item \textbf{Real-time Deployment}: Merging accuracy with computational performance when implementing real-time systems;
    \item \textbf{Interpretability}: The black-box nature of deep learning discourages traffic engineers from considering deep learning.
\end{itemize}

In this article a review will be presented that addresses the above challenges via an extensive comparison of various types of model architectures plus design/re-design based on performance.

\section{Related Work}

\subsection{Classic Machine Learning methods for predicting traffic}

Classic machine learning methods have been extensively used for traffic prediction. Methods such as Random Forests and Gradient Boosting have been able to capture non-linear relationships with traffic data. Using Support Vector Machines with different types of kernels (polynomial, radial) has been one of the most common models used for data classification. The use of these standard machine learning methods has necessitated a considerable amount of Feature engineering to obtain the appropriate data features for accurate prediction of traffic. Additionally, many of these traditional models may have difficulty dealing with temporal dependencies in traffic patterns.

\subsection{Deep Learning for Traffic Prediction}

Deep learning has revolutionized traffic prediction through its ability to automatically learn hierarchical features. Convolutional Neural Networks (CNNs) have been successfully applied to spatial-temporal traffic prediction, extracting local patterns from traffic flow matrices. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks capture temporal dependencies. Recent work on Graph Convolutional Networks (GCNs) models traffic as graph-structured data, achieving state-of-the-art results on benchmark datasets.

\subsection{New Approaches to Using Deep Learning for Forecasting
Traffic}

Deep learning has changed how we forecast traffic (and is largely the result of deep learning's ability to automatically learn superior hierarchical features from observed data) by using the quality and type of data monitored. For example, Convolutional Neural Networks (CNNs) have allowed us to identify local traffic patterns from spatio-temporal traffic matrixes. RNNs and LSTMs allow us to model temporal dependencies of the different measurements being taken. Graph Convolutional Networks (GCNs) allow us to represent our traffic data as graph-based data. The application of these different graph-based structures (graphs) to various datasets has allowed for the production of state-of-the-art traffic forecasts.

\subsection{Time-Series Data Augmentation}

Common time-series data augmentation techniques include: adding random noise (jittering); scaling; rotating; and permuting. One of the important attributes of traffic prediction augmentation is to maintain the dynamics of traffic while enhancing the diversity of the traffic data. Recent research has looked at how generative models (e.g., GANs and VAEs) can
help us generate synthetic traffic data; however, use of these types of models comes with the cost of additional complexity and training workload.

\section{Methodology}

\subsection{Dataset Overview}

This research utilizes real life traffic data collected at traffic intersections. The dataset contains 5,000 data points over a wide range of traffic and environmental conditions across three urban city locations. Characteristics of the dataset are shown in Table \ref{tab:dataset}.

\begin{table}[htbp]
\centering
\caption{Dataset Characteristics}
\label{tab:dataset}
\begin{tabular}{ll}
\toprule
\textbf{Attribute} & \textbf{Value} \\
\midrule
Total Samples & 5,000 \\
Raw Features & 10 \\
Engineered Features & 7 \\
Total Features & 17 \\
Target Classes & 4 \\
Class Labels & Low, Medium, High, Severe \\
Class Distribution & Balanced \\
Validation Method & 5-fold Stratified CV \\
Train-Test Ratio & 80-20 (per fold) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Engineering}

Feature engineering is essential for good model performance. We have identified and used 10 different raw features, and we generated 7 new features to help capture the dynamics of traffic movements:

\textbf{Raw Features (10):}
\begin{itemize}
    \item Junction ID (A, B, C)
    \item Vehicle counts (Cars, Bicycles, Buses, Trucks, Total)
    \item Weather Conditions, Temperature
    \item Hour of the Day, Day of the Week
\end{itemize}

\textbf{Engineered Features (7):}
\begin{itemize}
    \item \textbf{VehicleDensity} = Total Vehicles / (Cars + Bicycles + 1)
    \item \textbf{HeavyVehicleRatio} = (Buses + Trucks) / Total Vehicles
    \item \textbf{TimeOfDay}: Categorical Encoding
    \item \textbf{IsRushHour}: Binary (7:00 am - 9:00 am or 5:00 pm - 7:00 pm)
    \item \textbf{IsWeekend}: Binary
    \item \textbf{WeatherHourInteraction}: Cross-feature
    \item \textbf{JunctionRushHour}: Cross-feature
\end{itemize}

All categorical variables were label-encoded, and all continuous variables were standardized via StandardScaler.

\subsection{Data Augmentation Strategy}

To address the limited training data size, we employ Gaussian noise injection. For each training sample $\mathbf{x}$, we generate two augmented versions:

\begin{equation}
\mathbf{x}_{aug,i} = \mathbf{x} + \mathcal{N}(0, \sigma_i^2 \mathbf{I})
\end{equation}

where $\sigma_i \in \{0.02, 0.04\}$ represents noise levels, tripling the effective training set size.

\section{Architectural Design}

\subsection{Overview}

This paper presents a novel one-dimensional CNN architecture for predicting traffic using temporal tabular input data. Unlike standard two-dimensional CNN implementations that utilize two-dimensional imagery for feature extraction purposes, our architecture operates solely on temporal tabular input sequences containing 17 features each. A diagram depicting our entire architecture can be found in Figure 1 below.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{models/cnn_architecture.png}}
\caption{Proposed 1D CNN architecture with four convolutional blocks featuring progressive filter increase (128-256-384-512). Each block contains dual Conv1D layers with BatchNormalization, ReLU activation, and dropout regularization.}
\label{fig:architecture}
\end{figure}

\subsection{Architectural Design Principles}

There are four main design principles that define the architecture of our proposed design:

\textbf{1. Progressive Feature Extraction}: Four blocks of gradually increasing numbers of filters (128-256-384-512) provide hierarchical feature learning.

\textbf{2. Balanced Capacity}: A total of 2,500,000 parameters are included within the model to provide enough capacity without introducing problems related to overfitting.

\textbf{3. Computational Efficiency}: The use of one-dimensional convolutions allows us to achieve fast implementation times (¡10ms) for real time implementation.

\textbf{4. Regularization Strategy}: Use of Batch Normalization, progressive Drop Out (0.25-0.40), and Data Augmentation allows for robust generalization.

\subsection{Building Blocks of a Neural Network }

\textbf{Network Block 1 (Initial Feature Extraction Layer)}
\begin{itemize}
    \item Conv1D(128 filters, kernel=5, padding=same)
    \item BatchNormalization + ReLU
    \item Conv1D(128 filters, kernel=5, padding=same)
    \item BatchNormalization + ReLU
    \item MaxPooling1D(pool\_size=2)
    \item Dropout(0.25)
\end{itemize}

\textbf{Block 2-4}: The same architecture with 256 filters for block 2; 384 for block 3; 512 for block 4 without fully connecting.
Increasing dropout rates are experienced from blocks 2-4 by .05 of the previous block’s dropout rate (0.30, 0.35, and 0.40 respectively).

\textbf{Classification Head:}
\begin{itemize}
    \item GlobalAveragePooling1D
    \item Dense(768) + BatchNorm + ReLU + Dropout(0.40)
    \item Dense(384) + BatchNorm + ReLU + Dropout(0.35)
    \item Dense(192) + BatchNorm + ReLU + Dropout(0.30)
    \item Dense(4, activation=softmax)
\end{itemize}

\subsection{Baseline Models}

We compare our CNN against 8 baseline models.

\textbf{Traditional ML (5 total):} Decision Tree, Random Forest, Support Vector Machine, Logistic Regression, and Naïve Bayes.

\textbf{Transfer Learning (3 total):} VGG16-1D, VGG19-1D, ResNet50-1D (modified for 1-dimensional data).

\section{Implementation}

\subsection{Software Setup}

\textbf{Technology Stack:}
\begin{itemize}
    \item Python 3.13
    \item TensorFlow 2.20.0 / Keras API
    \item scikit-learn 1.5.0
    \item NumPy, Pandas \& Matplotlib
\end{itemize}

\textbf{Hardware:}
\begin{itemize}
    \item Intel Core i5 (CPU training)
    \item 8GB of RAM
    \item Windows 11
\end{itemize}

\subsection{Training Configuration}

All models use:
\begin{itemize}
    \item 5-fold Stratified Cross-Validation on validation datasets
    \item 200 epochs (with early stopping)
    \item 16 batch size
    \item Adam optimizer (lr = 0.0005)
    \item Categorical Cross-Entropy Loss
    \item \textbf{Callbacks}: ReduceLROnPlateau (factor = 0.5, patience = 7), Early Stopping (patience = 25)
\end{itemize}

\subsection{Code Structure}

Project files:
\begin{itemize}
    \item \textbf{train\_stable\_publication.py}: Main training script
    \item \textbf{dataset.py}: Data loading utilities
    \item \textbf{preprocess.py}: Feature engineering
    \item \textbf{app.py}: Streamlit web application
    \item \textbf{generate\_all\_figures.py}: Figure generation
\end{itemize}

\section{Experimental Results}

\subsection{Overall Comparison of Model Performance}

Table \ref{tab:overall} contains a comprehensive overview of all 9 model performances, while Figure \ref{fig:comparison} shows the visual performance of the models by way of an accuracy comparison.

\begin{table*}[t]
\centering
\caption{Performance Comparison: 5-Fold Cross-Validation Results}
\label{tab:overall}
\begin{tabular}{clcccc}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Accuracy (\%)} & \textbf{Std Dev} & \textbf{Precision (\%)} & \textbf{F1-Score (\%)} \\
\midrule
1 & \textbf{1D CNN (Proposed)} & \textbf{92.16} & $\pm$\textbf{0.72} & \textbf{92.22} & \textbf{92.16} \\
2 & Random Forest & 90.86 & $\pm$0.65 & 90.90 & 90.84 \\
3 & Decision Tree & 90.68 & $\pm$1.46 & 91.25 & 90.76 \\
4 & VGG16-1D & 89.28 & $\pm$1.01 & 89.49 & 89.22 \\
5 & VGG19-1D & 89.28 & $\pm$0.87 & 89.27 & 89.22 \\
6 & ResNet50-1D & 88.00 & $\pm$1.01 & 88.25 & 88.05 \\
7 & SVM & 87.02 & $\pm$0.80 & 87.40 & 87.04 \\
8 & Logistic Regression & 81.82 & $\pm$0.72 & 81.95 & 81.83 \\
9 & Naive Bayes & 79.28 & $\pm$0.49 & 78.95 & 79.07 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{models/model_comparison.png}
\caption{Accuracy comparison of all nine models with standard deviation error bars. The proposed 1D CNN achieves highest accuracy (92.16\%) with lowest variance ($\pm$0.72\%).}
\label{fig:comparison}
\end{figure}

1D CNN achieved the highest accuracy (92.16\%); performing 1.30\% better than the RF model, \& 2.88\% better than the VGG16-1D model.

\subsection{Model Performance by Folds}

Table \ref{tab:folds} presents 1D CNN accuracy for each fold.

\begin{table}[htbp]
\centering
\caption{1D CNN Accuracy per Fold}
\label{tab:folds}
\begin{tabular}{cc}
\toprule
\textbf{Fold} & \textbf{Accuracy (\%)} \\
\midrule
1 & 91.30 \\
2 & 93.40 \\
3 & 92.30 \\
4 & 94.00 \\
5 & 92.40 \\
\midrule
\textbf{Mean $\pm$ Std} & \textbf{92.68 $\pm$ 0.94} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Confusion Matrix Results}

Figure \ref{fig:confusion} presents the confusion matrix for the best fold (Fold 4: 94\%).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{models/confusion_matrix.png}
\caption{Confusion matrix for best fold (Fold 4, 94\% accuracy). Strong diagonal values indicate excellent classification across all four traffic severity levels (Low, Medium, High, Severe).}
\label{fig:confusion}
\end{figure}

The confusion matrix reveals:
\begin{itemize}
    \item High diagonal values: Correct predictions dominate
    \item Low off-diagonal confusion: Minimal misclassification
    \item Balanced performance across all classes
\end{itemize}

\subsection{The Effect of Data Augmentation}

Ablation Study Results:
\begin{itemize}
    \item \textbf{No augmentation}: 88.38\% $\pm$ 1.06\%
    \item \textbf{Data augmentation}: 92.16\% $\pm$ 0.72\%
    \item \textbf{Improvement}: +3.78\% absolute accuracy
\end{itemize}

\subsection{Analysis of Training Dynamics}

Figure \ref{fig:training} illustrates how the training and validation curves appear; it also indicates that all models converged smoothly with no overfitting.
\vspace{-1cm}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{models/training_curves.png}
\caption{Training and validation loss/accuracy curves over 200 epochs. Close alignment between training (blue) and validation (orange) curves indicates no overfitting. Model converges around epoch 150.}
\label{fig:training}
\end{figure}
% \vspace{5 cm}
% Key observations:
% \begin{itemize}
%     \item The models converged smoothly with no overfitting, 
%     \item Models showed signs of converging around epochs = 150, 
%     \item evidence of learning rate adaptations were noted as plateaus in the learning curves.
% \end{itemize}
% \vspace{1cm}
% \subsection{Computational Performance}

\begin{table}[htbp]
\centering
\caption{Computational Performance}
\label{tab:compute}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Training (min/fold)} & \textbf{Inference (ms)} \\
\midrule
1D CNN & 20-25 & 8.5 \\
VGG16-1D & 30-35 & 12.3 \\
ResNet50-1D & 40-45 & 18.7 \\
\bottomrule
\end{tabular}
\end{table}

% \section{Discussion}

% \subsection{The Reasons Why The Proposed CNN Outperforms Other Models}

% The main reasons are:

% 1. The architecture is specifically created for temporal data. 1D temporal data, not changed from 2D photo models. 

% 2. The total number of blocks (4) and the number of parameters (2.5 million). As a result, neither over-fitted nor under-fitted, yet there is sufficient capacity for the network to be able to handle those parameters  (2.5 Million). 

% 3. The regularization.  By using BatchNorm and using progressive Dropout while using some form of data augmentation will help to cover the overvisiting of these networks. 

% 4. The data augmentation.  In having 3.78\% increase via Gaussian noise injection was created through using augmented noise.

% \subsection{The Disadvantages of Transfer Learning}

% The under-performance is due to:
% \begin{itemize}
%     \item The structure of networks is produced for the 2D images only, not for the 1D sequences.
%     \item The inefficiencyal parameters are: ResNet50 (5.8 million) vs suggested network (2.5 million) 
%     \item There are no weights that can be used for pre-trained networks that can be applied to the traffic data for this application.
% \end{itemize}

% \subsection{Practical Implications}

% \begin{itemize}
%     \item \textbf{Real-time deployment}: 8.5ms inference enables 100+ Hz predictions
%     \item \textbf{High accuracy}: 92.16\% suitable for automated traffic management
%     \item \textbf{Consistency}: Low std ($\pm$0.72\%) ensures reliable predictions
%     \item \textbf{Scalability}: Manageable parameters (2.5M) enable scaling
% \end{itemize}

% \subsection{Limitations and Future Work}

% Future directions:
% \begin{enumerate}
%     \item Attention mechanisms for interpretability
%     \item Temporal sequences (past 15-30 minutes)
%     \item Multi-task learning (duration + severity)
%     \item Ensemble methods (CNN + Random Forest)
%     \item Large-scale urban deployment
% \end{enumerate}
\vspace{4cm}
\section{Conclusion}

This study compared nine different machine learning models to predict traffic flow. The proposed 1D CNN performed with an accuracy of 92.16\% ± 0.72\%, outperforms both traditional ML and transfer learning methods. 
Our major contributions include: (1) rigorous 5 fold cross-validation, (2) optimized CNN architecture (128-512 filters), (3) Gaussian noise augmentation (+3.78\% improvement), (4) evidence that task specific architectures outperform image adapted models, and (5) practical deployment ready system with 8.5ms of prediction time. Additionally, the high accuracy, consistency, and efficiency make the model well-suited for usage when developing real world intelligent transportation systems. 
Future efforts will examine attention mechanisms, multi-task learning, and large scale deployments.
\vspace{0.8cm}
\section{Abbreviations}

\begin{table}[htbp]
\centering
\caption{List of Abbreviations}
\begin{tabular}{ll}
\toprule
\textbf{Abbreviation} & \textbf{Full Form} \\
\midrule
AI & Artificial Intelligence \\
ARIMA & AutoRegressive Integrated Moving Average \\
CNN & Convolutional Neural Network \\
CV & Cross-Validation \\
DL & Deep Learning \\
DT & Decision Tree \\
GAN & Generative Adversarial Network \\
ITS & Intelligent Transportation Systems \\
LR & Logistic Regression \\
LSTM & Long Short-Term Memory \\
ML & Machine Learning \\
NB & Naive Bayes \\
ReLU & Rectified Linear Unit \\
RF & Random Forest \\
RNN & Recurrent Neural Network \\
SVM & Support Vector Machine \\
\bottomrule
\end{tabular}
\end{table}

\section*{Acknowledgment}

The authors would like to express their sincere gratitude to their faculty guide, P. V. V. S. Srinivas, for his continuous guidance, valuable suggestions, and constructive feedback throughout the course of this work. His expertise and encouragement played a significant role in shaping the direction and quality of the research. The authors also acknowledge the Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, for providing the necessary resources and a supportive research environment. Additionally, the authors extend their appreciation to their mentors and peers for their insightful discussions and feedback, which contributed to the refinement and improvement of this study.

\begin{thebibliography}{20}

\bibitem{ref1}
N. Parashuram and K. Vijayalakshmi, "A Comprehensive Analysis of Road Traffic Prediction Using Machine Learning Algorithms," 2024 First International Conference on Software, Systems and Information Technology (SSITCON), Tumkur, India, 2024, pp. 1-5. [Online]. Available: https://ieeexplore.ieee.org/document/10797035

\bibitem{ref2}
P. Sun, N. Aljeri and A. Boukerche, "Machine Learning-Based Models for Real-time Traffic Flow Prediction in Vehicular Networks," in IEEE Network, vol. 34, no. 3, pp. 178-185, May/June 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9061003.

\bibitem{ref3}
T. Axcellin and S. Kamalakkannan, "Machine Learning-Based Traffic Flow Prediction and Management," 2025 6th International Conference on Inventive Research in Computing Applications (ICIRCA), Coimbatore, India, 2025, pp. 1320-1324. [Online]. Available: https://ieeexplore.ieee.org/document/11089788

\bibitem{ref4}
V. D. Chaudhari, A. J. Patil, D. J. Shirale, T. R. Al-Shaikhli, A. V. Kumar and B. Eswaran, "Improving Traffic Flow in Smart Cities with Machine Learning-Based Traffic Management," 2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM), Chennai, India, 2024, pp. 1-5. [Online]. Available: https://ieeexplore.ieee.org/document/10568728.

\bibitem{ref5}
A. Bouhali, A. Zeroual and F. Harrou, "Enhancing Traffic Flow Prediction with Machine Learning Models: A Comparative Study," 2024 International Conference of the African Federation of Operational Research Societies (AFROS), Tlemcen, Algeria, 2024, pp. 1-6. [Online]. Available: https://ieeexplore.ieee.org/document/11036955

\bibitem{ref6}
P. Kumar Pareek, V. K, N. S, G. G and D. H A, "Forecasting of Traffic Flow Using Feature Selection with ML Model," 2023 International Conference on Data Science and Network Security (ICDSNS), Tiptur, India, 2023, pp. 1-6. [Online]. Available: https://ieeexplore.ieee.org/document/10245394

\bibitem{ref7}
M. Yadav and B. Kumar, "Comparative Analysis of Machine Learning Algorithms for Traffic Flow Prediction in Intelligent Transportation Systems," 2024 4th International Conference on Technological Advancements in Computational Sciences (ICTACS), Tashkent, Uzbekistan, 2024, pp. 1752-1755. [Online]. Available: https://ieeexplore.ieee.org/document/10840512

\bibitem{ref8}
M. Saisree Medikonduru, A. Devadari, S. Chowdary Kasaraneni, M. Bhavani Thota and S. Harsha Yakkala, "Traffic Prediction for an Intelligent Transportation System using ML," 2022 International Conference on Inventive Computation Technologies (ICICT), Nepal, 2022, pp. 1206-1213. [Online]. Available: https://ieeexplore.ieee.org/document/9850574

\bibitem{ref9}
S. A. Hafeez and M. R, "Intelligent Traffic Flow Prediction: A CNN-LSTM Hybrid Model with Bio-Inspired Fine-Tuning Using Marine Predator Algorithm," 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE), Virudhunagar, India, 2025, pp. 1-6. [Online]. Available: https://ieeexplore.ieee.org/document/11053029

\bibitem{ref10}
C. Johny and V. Dahiya, "Machine Learning Applications in Vehicular Traffic Prediction and Congestion Control: A Systematic Review," 2022 6th International Conference on Electronics, Communication and Aerospace Technology, Coimbatore, India, 2022, pp. 948-954. [Online]. Available: https://ieeexplore.ieee.org/document/10009384

\bibitem{ref11}
A. Kulkarni, P. Anitha, J. Y. Valluri, M. V. Sunena Rose, U. Hemavathi and O. M. Hussein, "Machine Learning Approaches for Efficient Traffic Flow in Smart Cities," 2024 3rd Odisha International Conference on Electrical Power Engineering, Communication and Computing Technology (ODICON), Bhubaneswar, India, 2024, pp. 1-4. [Online]. Available: https://ieeexplore.ieee.org/document/10797496

\bibitem{ref12}
L. Xia, "Design of Urban Road Traffic Induction Algorithm based on DL Algorithm," 2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI), Changchun, China, 2022, pp. 1213-1216. [Online]. Available: https://ieeexplore.ieee.org/document/9832120

\bibitem{ref13}
H. Liu, Y. Lin, Z. Chen, D. Guo, J. Zhang and H. Jing, "Research on the Air Traffic Flow Prediction Using a Deep Learning Approach," in IEEE Access, vol. 7, pp. 148019-148030, 2019. [Online]. Available: https://ieeexplore.ieee.org/document/8861045

\bibitem{ref14}
Y. Gu, W. Lu, X. Xu, L. Qin, Z. Shao and H. Zhang, "An Improved Bayesian Combination Model for Short-Term Traffic Prediction With Deep Learning," in IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 3, pp. 1332-1342, March 2020. [Online]. Available: https://ieeexplore.ieee.org/document/8842618

\bibitem{ref15}
Z. Pan et al., "Spatio-Temporal Meta Learning for Urban Traffic Prediction," in IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 3, pp. 1462-1476, 1 March 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9096591

\bibitem{ref16}
K. Guo et al., "Optimized Graph Convolution Recurrent Neural Network for Traffic Prediction," in IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 2, pp. 1138-1149, Feb. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/8959420

\bibitem{ref17}
Z. Zhou, Y. Qin and H. Luo, "Deep Spatio-Temporal Convolutional Neural Network for City Traffic Flow Prediction," 2021 2nd International Conference on Computing and Data Science (CDS), Stanford, CA, USA, 2021, pp. 171-175. [Online]. Available: https://ieeexplore.ieee.org/document/9463301

\bibitem{ref18}
J. Zheng and M. Huang, "Traffic Flow Forecast Through Time Series Analysis Based on Deep Learning," in IEEE Access, vol. 8, pp. 82562-82570, 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9079512

\bibitem{ref19}
G. Meena, D. Sharma and M. Mahrishi, "Traffic Prediction for Intelligent Transportation System using Machine Learning," 2020 3rd International Conference on Emerging Technologies in Computer Engineering: Machine Learning and Internet of Things (ICETCE), Jaipur, India, 2020, pp. 145-148. [Online]. Available: https://ieeexplore.ieee.org/document/9091758

\bibitem{ref20}
A. Koesdwiady, R. Soua and F. Karray, "Improving Traffic Flow Prediction With Weather Information in Connected Cars: A Deep Learning Approach," in IEEE Transactions on Vehicular Technology, vol. 65, no. 12, pp. 9508-9517, Dec. 2016. [Online]. Available: https://ieeexplore.ieee.org/document/7501574

\end{thebibliography}

\end{document}
