"""
NOVEL COMPONENT: Attention-Enhanced 1D CNN for Traffic Prediction
This is YOUR unique contribution for publication
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras import layers, models
import matplotlib.pyplot as plt
from pathlib import Path

class AttentionLayer(layers.Layer):
    """
    Custom Attention Mechanism for Traffic Feature Importance
    This is the NOVEL component that makes your work unique!
    """
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight',
                                  shape=(input_shape[-1], input_shape[-1]),
                                  initializer='glorot_uniform',
                                  trainable=True)
        self.b = self.add_weight(name='attention_bias',
                                  shape=(input_shape[-1],),
                                  initializer='zeros',
                                  trainable=True)
        super(AttentionLayer, self).build(input_shape)
    
    def call(self, inputs):
        # Calculate attention scores
        attention_scores = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        
        # Apply attention
        attended_features = inputs * attention_weights
        return attended_features
    
    def get_config(self):
        return super(AttentionLayer, self).get_config()


def create_attention_cnn(input_shape, n_classes):
    """
    Novel Attention-Enhanced 1D CNN Architecture
    
    Key Innovation:
    - Self-attention mechanism to learn feature importance
    - Multi-scale feature extraction
    - Residual connections for better gradient flow
    
    This is YOUR contribution to the field!
    """
    inputs = layers.Input(shape=input_shape)
    
    # Block 1: Initial feature extraction
    x = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x1 = layers.Conv1D(64, 3, activation='relu', padding='same')(x)
    x1 = layers.BatchNormalization()(x1)
    
    # Attention mechanism on first block
    x1_attended = AttentionLayer()(x1)
    x1 = layers.Add()([x1, x1_attended])  # Residual connection
    x1 = layers.MaxPooling1D(2)(x1)
    x1 = layers.Dropout(0.3)(x1)
    
    # Block 2: Deeper features
    x2 = layers.Conv1D(128, 3, activation='relu', padding='same')(x1)
    x2 = layers.BatchNormalization()(x2)
    x2 = layers.Conv1D(128, 3, activation='relu', padding='same')(x2)
    x2 = layers.BatchNormalization()(x2)
    
    # Attention mechanism on second block
    x2_attended = AttentionLayer()(x2)
    x2 = layers.Add()([x2, x2_attended])  # Residual connection
    x2 = layers.MaxPooling1D(2)(x2)
    x2 = layers.Dropout(0.4)(x2)
    
    # Block 3: High-level features
    x3 = layers.Conv1D(256, 3, activation='relu', padding='same')(x2)
    x3 = layers.BatchNormalization()(x3)
    
    # Global attention aggregation
    x3_attended = AttentionLayer()(x3)
    x3 = layers.Add()([x3, x3_attended])
    
    # Global pooling
    x_gap = layers.GlobalAveragePooling1D()(x3)
    x_gmp = layers.GlobalMaxPooling1D()(x3)
    x = layers.Concatenate()([x_gap, x_gmp])
    
    # Dense layers
    x = layers.Dense(512, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    
    # Output
    outputs = layers.Dense(n_classes, activation='softmax')(x)
    
    model = models.Model(inputs=inputs, outputs=outputs, name='Attention_CNN')
    return model


def create_hybrid_ensemble(input_shape, n_classes):
    """
    Hybrid Ensemble: CNN + Traditional ML Features
    Another novel contribution!
    """
    inputs = layers.Input(shape=input_shape)
    
    # CNN branch
    cnn_branch = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
    cnn_branch = layers.BatchNormalization()(cnn_branch)
    cnn_branch = AttentionLayer()(cnn_branch)
    cnn_branch = layers.GlobalAveragePooling1D()(cnn_branch)
    
    # Statistical features branch
    stat_branch = layers.Flatten()(inputs)
    stat_branch = layers.Dense(128, activation='relu')(stat_branch)
    stat_branch = layers.BatchNormalization()(stat_branch)
    
    # Merge branches
    merged = layers.Concatenate()([cnn_branch, stat_branch])
    x = layers.Dense(256, activation='relu')(merged)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    
    outputs = layers.Dense(n_classes, activation='softmax')(x)
    
    model = models.Model(inputs=inputs, outputs=outputs, name='Hybrid_Ensemble')
    return model


# Visualization function for paper
def visualize_attention_weights(model, X_sample, feature_names, save_path='publication_results/attention_visualization.png'):
    """
    Visualize what the attention mechanism learned
    Perfect for publication figures!
    """
    # Get attention layer outputs
    attention_layers = [layer for layer in model.layers if isinstance(layer, AttentionLayer)]
    
    if not attention_layers:
        print("No attention layers found")
        return
    
    # Create attention model
    attention_model = models.Model(inputs=model.input,
                                   outputs=[layer.output for layer in attention_layers])
    
    # Get attention weights
    attention_outputs = attention_model.predict(X_sample[:100])
    
    # Plot
    fig, axes = plt.subplots(len(attention_layers), 1, figsize=(12, 4*len(attention_layers)))
    if len(attention_layers) == 1:
        axes = [axes]
    
    for idx, (ax, att_output) in enumerate(zip(axes, attention_outputs)):
        # Average attention across samples
        avg_attention = np.mean(np.abs(att_output), axis=0).flatten()
        
        # Plot
        ax.bar(range(len(avg_attention)), avg_attention, color='steelblue', alpha=0.7)
        ax.set_title(f'Attention Layer {idx+1}: Feature Importance', fontsize=14, fontweight='bold')
        ax.set_xlabel('Feature Dimension', fontsize=12)
        ax.set_ylabel('Attention Weight', fontsize=12)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ Attention visualization saved: {save_path}")
    plt.close()


if __name__ == "__main__":
    print("="*80)
    print("ðŸŽ¯ NOVEL COMPONENT: Attention-Enhanced CNN")
    print("="*80)
    
    # Example usage
    input_shape = (17, 1)  # 17 features
    n_classes = 4  # 4 traffic levels
    
    # Create novel model
    model = create_attention_cnn(input_shape, n_classes)
    model.summary()
    
    print("\nâœ… Novel architecture created!")
    print("ðŸ“Š Key innovations:")
    print("   1. Self-attention mechanism for feature importance")
    print("   2. Multi-scale feature extraction with residual connections")
    print("   3. Dual-pooling aggregation (Average + Max)")
    print("\nðŸŽ¯ This is YOUR unique contribution for publication!")
